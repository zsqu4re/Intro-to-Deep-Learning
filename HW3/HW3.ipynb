{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `NotImplemented` parts of the code cells and write your answers in the markdown cells designated for your response to any questions asked. The tag `# AUTOGRADED` (all caps, with a space after `#`) should be at the beginning of each autograded code cell, so make sure that you do not change that. You are also not allowed to import any new package other than the ones already imported. Doing so will prevent the autograder from grading your code.\n",
    "\n",
    "For the code submission, run the last cell in the notebook to create the submission zip file. If you are working in Colab, make sure to download and then upload a copy of the completed notebook itself to its working directory to be included in the zip file. Finally, submit the zip file to Gradescope.\n",
    "\n",
    "If you are running the notebook locally, make sure you have created a virtual environment (using `conda` for example) and have the proper packages installed. We are working with `python=3.10` and `torch>=2`.\n",
    "\n",
    "Files to be included in submission:\n",
    "\n",
    "- `HW3.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a 2D convolution function\n",
    "This week you learned about the convolution operation, which is the central operation of Convolutional Neural Networks (CNNs). In this homework, you are going to implement a 2D convolution function using numpy, so that you understand its nuts and bolts, as well as some extended variations of the convolution: strided, dilated, and grouped convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL OR ADD ANY ADDITIONAL IMPORTS!\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from typing import Tuple, Union\n",
    "\n",
    "# For interactive plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except ImportError:\n",
    "    pass\n",
    "try:\n",
    "    %matplotlib widget\n",
    "except:\n",
    "    %pip install ipympl\n",
    "    %matplotlib widget\n",
    "\n",
    "# a helper function to convert a single value to a tuple:\n",
    "def to_tuple(\n",
    "        x: Union[int, Tuple[int, int]]\n",
    "        ) -> Tuple[int, int]:\n",
    "    \n",
    "    if isinstance(x, tuple):\n",
    "        return x\n",
    "    return (x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding, Stride, and Dilation\n",
    "\n",
    "In the simple convolution, the convolution kernel (filter) is applied to different locations in the input like a sliding window. By 'applied' we mean that the inner product (sum of the element-wise multiplication) of the kernel with the chosen window of the input is calculated, and the result is stored to the corresponding location in the output. As the inner product of two vectors show how similar their direction is, you can think of the convolution as a pattern matching that is done everywhere in the input. \n",
    "\n",
    "In order to increase the convenience of convolution in certain aspects, several modifications can be applied:\n",
    "\n",
    "- Padding: padding is extending the input over its edges, which is usually done to keep the output shape the same as input. Without padding, the output shape is going to be smaller, which may sometimes be undesirable. The values at the extentions over the edges are arbitrary, but the default is 0 (zero-padding). Usually the padding size is the same on both ends of each axis, but may be different for each axis.\n",
    "\n",
    "- Stride: the stride is the step that the sliding window takes each time over the input. By increasing the stride, the pattern matching becomes less exhaustive, and the output will have a smaller shape. For example, if stride is 2, the sliding window skips every other index, and the output shape in each axis is almost halved.\n",
    "\n",
    "- Dilation: while the simple convolution applies the kernel to a compact window, the sliding window of a dilated convolution is dilated. This may be understood more easily with visualization in the link that we provide below.\n",
    "\n",
    "You can look at [Convolution Arithmetic Visualizations](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) to better understand these concepts. We are not discussing transposed convolution here. The animation for the dilated convolution (the last animation) corresponds to `dilation=2` in each axis. The dilation can be any positive integer, and your code should work for any dilation.\n",
    "\n",
    "Now, you have to implement a code to correctly calculate the output shape and the slice of the input that is used the calculate the output at location `(i, j)`, based on the input shape, padding, stride, and dilation.\n",
    "\n",
    "For each of these arguments, you can pass one integer (which will be used for both axes), or a tuple that contains the value for each axis.\n",
    "\n",
    "Below you will find an interactive plotting class that can help you visualize a convolution configuration. The two parts that you have to implement are marked with `NotImplemented`. After you fill in your code, run the cell after to get the interactive plot. If your code is correct, you should not get any error for any output location, and you should be able to traverse all the output locations and all the possible input locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvViz:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape = Union[int, Tuple[int, int]],\n",
    "        kernel_shape = Union[int, Tuple[int, int]],\n",
    "        stride = Union[int, Tuple[int, int]],\n",
    "        padding = Union[int, Tuple[int, int]],\n",
    "        dilation = Union[int, Tuple[int, int]],\n",
    "        ):\n",
    "        self.X_in, self.Y_in = to_tuple(input_shape)\n",
    "        self.X_k, self.Y_k = to_tuple(kernel_shape)\n",
    "        self.sx, self.sy = to_tuple(stride)\n",
    "        self.px, self.py = to_tuple(padding)\n",
    "        self.dx, self.dy = to_tuple(dilation)\n",
    "\n",
    "        # Padded input shape:\n",
    "        # YOUR CODE\n",
    "        self.padded_shape = (NotImplemented, NotImplemented)\n",
    "\n",
    "        # Output shape:\n",
    "        # YOUR CODE\n",
    "        self.X_out = NotImplemented\n",
    "        self.Y_out = NotImplemented\n",
    "\n",
    "        assert self.X_out > 0 and self.Y_out > 0, 'Convolutional not possible'\n",
    "\n",
    "    def get_input_slice(\n",
    "        self,\n",
    "        i: int,\n",
    "        j: int,\n",
    "        ) -> Tuple[slice, slice]:\n",
    "        # Find the correct slice for the input tensor to be applied to the kernel size\n",
    "        # use slice(start, end, step) to define the slice for x and y axis.\n",
    "\n",
    "        # YOUR CODE\n",
    "        NotImplemented\n",
    "\n",
    "        return slice(NotImplemented), slice(NotImplemented)\n",
    "\n",
    "    def get_field(self, i, j):\n",
    "\n",
    "        # creates the input field with values:\n",
    "        # 0.5 for the padded area --> white\n",
    "        # 0 for the non-padded area --> blue\n",
    "        # 1 for the convolved area --> red\n",
    "        plot_input = np.ones(self.padded_shape, dtype=np.float32)/2\n",
    "        plot_input[self.px:-self.px, self.py:-self.py] = 0\n",
    "        input_slice = self.get_input_slice(i, j)\n",
    "        plot_input[input_slice] = 1\n",
    "\n",
    "        # creates the output field with values:\n",
    "        # 0 for all values except the current output value --> blue\n",
    "        # 1 for the current output value --> red\n",
    "        plot_output = np.zeros((self.X_out, self.Y_out), dtype=np.float32)\n",
    "        plot_output[i, j] = 1\n",
    "\n",
    "        return plot_input, plot_output\n",
    "\n",
    "    def show(self):\n",
    "        plot_input, plot_output = self.get_field(0, 0)\n",
    "        self.fig, self.ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        self.in_img = self.ax[0].imshow(plot_input.T, cmap='bwr')\n",
    "        self.out_img = self.ax[1].imshow(plot_output.T, cmap='bwr')\n",
    "        self.ax[0].set_title('Input')\n",
    "        self.ax[1].set_title('Output')\n",
    "        self.ax[0].set_xlabel('x')\n",
    "        self.ax[0].set_ylabel('y')\n",
    "        self.ax[1].set_xlabel('x')\n",
    "        self.ax[1].set_ylabel('y')\n",
    "        self.ax[0].set_xticks(np.arange(self.X_in+2*self.px)-0.5)\n",
    "        self.ax[0].set_yticks(np.arange(self.Y_in+2*self.py)-0.5)\n",
    "        self.ax[1].set_xticks(np.arange(self.X_out)-0.5)\n",
    "        self.ax[1].set_yticks(np.arange(self.Y_out)-0.5)\n",
    "        self.ax[0].grid(color='black')\n",
    "        self.ax[1].grid(color='black')\n",
    "        self.ax[0].set_xticklabels(np.arange(self.X_in+2*self.px))\n",
    "        self.ax[0].set_yticklabels(np.arange(self.Y_in+2*self.py))\n",
    "        self.ax[1].set_xticklabels(np.arange(self.X_out))\n",
    "        self.ax[1].set_yticklabels(np.arange(self.Y_out))\n",
    "        self.i = widgets.IntSlider(min=0, max=self.X_out-1, description='i', layout={'width': '300px'})\n",
    "        self.j = widgets.IntSlider(min=0, max=self.Y_out-1, description='j', layout={'width': '300 px'})\n",
    "        widgets.interact(self.update, i=self.i, j=self.j)\n",
    "\n",
    "    def update(self, i, j):\n",
    "        plot_input, plot_output = self.get_field(i, j)\n",
    "        self.in_img.set_data(plot_input.T)\n",
    "        self.out_img.set_data(plot_output.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the arguments to arbitrary values to test your code\n",
    "conv_viz = ConvViz(\n",
    "    input_shape = (10, 10), \n",
    "    kernel_shape = (3, 2), \n",
    "    stride = (2, 2), \n",
    "    padding = (2, 2), \n",
    "    dilation = (2, 2),\n",
    "    )\n",
    "conv_viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in `conv2d` (90)\n",
    "\n",
    "Now that you have learned about padding, stride, and dilation, you can implement a 2D convolution. Your task is to complete the function provided in the next cell only using `numpy`. After you are done, you can use `torch.nn.functional.conv2d` to test your function in the two cells after. \n",
    "\n",
    "Here, we introduce another modified version of colvolution called grouped convolution, which is about the input and output channels. In the default convolution, each output channels has its own kernel of shape $(c_{in}, X_k, Y_k)$ and the sliding window is actually a 3D cube over all input channels and the spatial window. Therefore, the full kernel is of shape $(c_{out}, c_{in}, X_k, Y_k)$. In grouped convolution, the input and output channels are split into several groups and each group is calculated like an independent default convolution. You can read [`torch.nn.Conv2D()` documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for a better understanding. The kernel of each group will be of shape $(\\dfrac{c_{out}}{n_{groups}}, \\dfrac{c_{in}}{n_{groups}}, X_k, Y_k)$, and by putting them together, the full kernel is of shape $(c_{out}, \\dfrac{c_{in}}{n_{groups}}, X_k, Y_k)$.\n",
    "\n",
    "Note: You should not use any `for` loops other than the ones provided. In general, avoiding `for` loops is important to increase the efficiency of your code. Each `for` loop will have a penalty of -5 on your grade. Some hints are provided for you on how to work with the tensors to be able to implement the convolution without explicit loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOGRADED\n",
    "\n",
    "def conv2d(\n",
    "        Input: np.ndarray, # shape: (c_in, X_in, Y_in)\n",
    "        Kernel: np.ndarray, # shape: (c_out, c_in_per_group, X_kernel, Y_kernel)\n",
    "        Bias: np.ndarray = np.array(0.), # shape: (c_out,) or (1,) or () (broadcastable to (c_out,))\n",
    "\n",
    "        stride: Union[int, Tuple[int, int]] = 1,\n",
    "        padding: Union[int, Tuple[int, int]] = 0,\n",
    "        dilation: Union[int, Tuple[int, int]] = 1,\n",
    "        groups: int = 1,\n",
    "\n",
    "        ) -> np.ndarray: # shape: (c_out, X_out, Y_out)\n",
    "\n",
    "    sx, sy = to_tuple(stride)\n",
    "    px, py = to_tuple(padding)\n",
    "    dx, dy = to_tuple(dilation)\n",
    "\n",
    "    c_in, X_in, Y_in = Input.shape\n",
    "    c_out, c_in_per_group, X_k, Y_k = Kernel.shape\n",
    "\n",
    "    assert groups*c_in_per_group == c_in, f\"groups * c_in_per_group == c_in should hold ({groups} * {c_in_per_group} != {c_in})\"\n",
    "    assert c_out % groups == 0, f\"c_out % groups == 0 should hold ({c_out} % {groups} != 0)\"\n",
    "\n",
    "    # Calculate the output dimensions (in x and y axes)\n",
    "\n",
    "    # YOUR CODE\n",
    "    X_out = NotImplemented\n",
    "    Y_out = NotImplemented\n",
    "\n",
    "    assert X_out > 0 and Y_out > 0, \"Convolution not possible\"\n",
    "\n",
    "    # Pad the input. (you can use np.pad). Look up the documentation to see how to use it.\n",
    "    # YOUR CODE\n",
    "    Input = np.pad(NotImplemented) \n",
    "\n",
    "    # Group the input and output channels in the input and kernel:\n",
    "    # HINT: if you want to group a dimension of size N such that it becomes G groups, each with N//G elements,\n",
    "    # where the first group is [0:N//G], the second is [N//G:2*N//G], etc. you can reshape that dimension to (G, N//G).\n",
    "    # YOUR CODE\n",
    "    Input = NotImplemented\n",
    "    c_out_per_group = c_out // groups\n",
    "    Kernel = NotImplemented\n",
    "\n",
    "    # Initialize the output with dtype np.float32 (The shape is your choice. choose wisely!)\n",
    "    # YOUR CODE\n",
    "    Output = np.empty(NotImplemented, dtype=np.float32)\n",
    "\n",
    "    # Calculate the output elements one location at a time (sliding window over input)\n",
    "    for i in range(X_out):\n",
    "        for j in range(Y_out):\n",
    "            # Calculating output at position (i, j)\n",
    "        \n",
    "            # select the correct slice of the input for the calculation of this output (call it Input_slice)\n",
    "            # YOUR CODE\n",
    "            NotImplemented\n",
    "\n",
    "            Input_slice = Input[NotImplemented]\n",
    "\n",
    "            # Calculate the convolution and assign it to the output element at position (i, j)\n",
    "            # HINT: you may benefit from a dummy dimension somewhere to make some operation broadcastable\n",
    "            # YOUR CODE\n",
    "            Output[NotImplemented] = NotImplemented\n",
    "\n",
    "\n",
    "    # Reshape the output to final correct shape (c_out, X_out, Y_out)\n",
    "    # YOUR CODE\n",
    "    Output = Output.reshape(c_out, X_out, Y_out)\n",
    "    \n",
    "    # Add the bias across output channel dimension (broadcast across the other dimensions)\n",
    "    # Be careful. Bias might be of shape () or (1,) or (c_out,).\n",
    "    # YOUR CODE\n",
    "    Output = NotImplemented\n",
    "\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARBITRARY SETTINGS FOR TESTING:\n",
    "\n",
    "# Input and output \n",
    "c_in , X_in, Y_in = 32, 28, 28\n",
    "c_out = 64\n",
    "\n",
    "# Kernel size:\n",
    "X_k, Y_k = 3, 3\n",
    "\n",
    "# Convolution hyperparameters:\n",
    "stride = (1, 1)\n",
    "padding = (0, 0)\n",
    "dilation = (1, 1)\n",
    "groups = 1\n",
    "\n",
    "assert c_in % groups == 0, \"c_in should be divisible by groups\"\n",
    "assert c_out % groups == 0, \"c_out should be divisible by groups\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING WITH RANDOM INPUTS:\n",
    "\n",
    "# Run this cell several times to test your function's output\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Random input, kernel and bias\n",
    "Input = np.random.randn(c_in, X_in, Y_in).astype(np.float32)\n",
    "Kernel = np.random.randn(c_out, c_in//groups, X_k, Y_k).astype(np.float32)\n",
    "Bias = np.random.randn(c_out).astype(np.float32)\n",
    "\n",
    "# Calculate the output using your function\n",
    "your_output = conv2d(Input, Kernel, Bias, stride, padding, dilation, groups)\n",
    "\n",
    "# Calculate the output using PyTorch\n",
    "Input_torch = torch.tensor(Input[None, ...]) # A dummy dimension as the batch dimension\n",
    "Kernel_torch = torch.tensor(Kernel)\n",
    "Bias_torch = torch.tensor(Bias)\n",
    "torcX_output = F.conv2d(Input_torch, Kernel_torch, Bias_torch, stride=stride, padding=padding, dilation=dilation, groups=groups)[0]\n",
    "\n",
    "# Calculate the relative L2 norm error\n",
    "# If your output is of the wrong shape, this will raise an error\n",
    "torch.testing.assert_close(torch.as_tensor(your_output), torcX_output, rtol=1e-3, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement average pooling using `conv2d` (10)\n",
    "\n",
    "Pooling is a simple way to reduce the memory cost of convolutional neural networks. The most common types of pooling layers are average pooling and max pooling, where the average or maximum of the elements in the sliding window is calculated as the corresponding output for that window. You are going to implement average pooling as a special case of convolution. You will have to define the convolution filter based on the pooling kernel shape. Pooling is done per channel, meaning that each output channel cororesponds to the input channel. After you are done, you will use `torch.nn.functional.avg_pool2d` to test your function. Remember that you can only use `numpy`, not `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOGRADED\n",
    "\n",
    "def avg_pool2d(\n",
    "        Input: np.ndarray, # shape: (c, X_in, Y_in)\n",
    "        kernel_size: Union[int, Tuple[int, int]],\n",
    "        stride: Union[int, Tuple[int, int]],\n",
    "        padding: Union[int, Tuple[int, int]],\n",
    "        ):\n",
    "    \n",
    "    c, X_in, Y_in = Input.shape\n",
    "    X_k, Y_k = to_tuple(kernel_size)\n",
    "    sx, sy = to_tuple(stride)\n",
    "    px, py = to_tuple(padding)\n",
    "\n",
    "    # Design the pooling kernel based on input and kernel size\n",
    "    # YOUR CODE\n",
    "    pool_kernel = NotImplemented\n",
    "\n",
    "    # Calculate the output using your conv2d function and the correct arguments\n",
    "    # YOUR CODE\n",
    "    output = conv2d(NotImplemented)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test config\n",
    "\n",
    "c = 5\n",
    "X_in, Y_in = 13, 15\n",
    "kernel_size = (4, 4)\n",
    "stride = (3, 2)\n",
    "padding = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with random inputs (run this cell several times to test your function)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Input = np.random.randn(c, X_in, Y_in)\n",
    "your_output = avg_pool2d(Input, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "Input_torch = torch.tensor(Input[None, ...])\n",
    "torch_output = F.avg_pool2d(Input_torch, kernel_size=kernel_size, stride=stride, padding=padding)[0]\n",
    "\n",
    "torch.testing.assert_close(torch.as_tensor(your_output), torch_output, atol=1e-3, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip submission files\n",
    "\n",
    "You can run the following cell to zip the generated files for submission.\n",
    "\n",
    "If you are on Colab, you do not need to have your drive mounted for this. You can just download the notebook file via the download option in the File dropdown menu on the top left, and upload it inside the directory of the notebook runtime (in the Files tab on the left, where the sample_data folder and the generated files are). This is the directory that the notebook has access to. After you upload the notebook file as well, run the next cell to zip the notebook and the submission files and download it for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_files(output_filename, *file_paths):\n",
    "    with zipfile.ZipFile(output_filename, 'w') as zipf:\n",
    "        for file_path in file_paths:\n",
    "            zipf.write(file_path, os.path.basename(file_path))\n",
    "\n",
    "files_to_zip = ['HW3.ipynb']\n",
    "output_zip = 'HW3_submission.zip'\n",
    "zip_files(output_zip, *files_to_zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
