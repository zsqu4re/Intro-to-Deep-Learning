{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5890b9d",
   "metadata": {},
   "source": [
    "Please complete the `NotImplemented` parts of the code cells and write your answers in the markdown cells designated for your response to any questions asked. The tag `# AUTOGRADED` (all caps, with a space after `#`) should be at the beginning of each autograded code cell, so make sure that you do not change that. You are also not allowed to import any new package other than the ones already imported. Doing so will prevent the autograder from grading your code.\n",
    "\n",
    "For the code submission, run the last cell in the notebook to create the submission zip file. If you are working in Colab, make sure to download and then upload a copy of the completed notebook itself to its working directory to be included in the zip file. Finally, submit the zip file to Gradescope.\n",
    "\n",
    "After you finish the assignment and fill in your code and response where needed (all cells should have been run), save the notebook as a PDF using the `jupyter nbconvert --to pdf HW2.ipynb` command (via a notebook code cell or the command line directly) and submit the PDF to Gradescope under the PDF submission item. If you cannot get this to work locally, you can upload the notebook to Google Colab and create the PDF there. You can find the notebook containing the instruction for this on Canvas.\n",
    "\n",
    "If you are running the notebook locally, make sure you have created a virtual environment (using `conda` for example) and have the proper packages installed. We are working with `python=3.10` and `torch>=2`.\n",
    "\n",
    "Files to be included in submission:\n",
    "\n",
    "- `HW2.ipynb`\n",
    "\n",
    "- `model_config.yaml`\n",
    "\n",
    "- `train_config.yaml`\n",
    "\n",
    "- `state_dict.pth`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff320aae",
   "metadata": {},
   "source": [
    "# Build and train a neural network for regression\n",
    "\n",
    "The problem you are asked to solve is Airfoil Self-Noise prediction. Namely, given 5 features (Frequency in Hertz, Angle of attack in degrees, Chord length in meters, Free-stream velocity in meters per second, and Suction side displacement thickness in meters), your model is supposed to accurately predict the Scaled sound pressure level, in decibels. The datasets have been preprocessed for you and can be found as `train.npy` and `val.npy` in the `data` folder. You have to implement your custom dataset, model, and train function. We have also provided helper functions for you to keep track of model performance during training. Please make use of them, and try to understand their code as you may need to implement similar functions in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL OR ADD ANY IMPORTS IN OTHER CELLS!\n",
    "\n",
    "from typing import Union, Tuple, List, Sequence\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from HW2_utils import save_yaml, load_yaml, zip_files, Learning_Curve_Tracker\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    Device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    Device = 'mps'\n",
    "else:\n",
    "    Device = 'cpu'\n",
    "\n",
    "print(f'Device is {Device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af92867",
   "metadata": {},
   "source": [
    "## Implement the dataset class (20)\n",
    "First, you will implement a subclass of `torch.utils.data.Dataset` to define a custom dataset class. To do so, you will need to implement three methods for the subclass:\n",
    "\n",
    "- `__init__` defines the dataset using the path to the data file (for example `data/train.npy` or `data/val.npy`). Your code should load the data using `np.load` and save it as attributes to be refereced in other methods that you implement. You can apply transformations like changing the dtype of data when saving them as attributes, which might be convenient.\n",
    "\n",
    "- `__len__` should return a non-negative integer that is the total number of data points. This will be used by the dataloader to count and batch the data.\n",
    "\n",
    "- `__getitem__` should return a single data sample (containing input, output pairs for this problem) using the index passed. Generally, the `__getitem__` method defines the behavior of an object when indexed using square brackets (like `a[i]`). \n",
    "\n",
    "Both datasets are of shape `(N, 6)` where N is the number of samples. The first five indexes of the last dimension contain the input features and the last one contains the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirFoilDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        data = np.load(data_path)\n",
    "        # process the data as torch tensors with the correct dtype and shape\n",
    "        NotImplemented\n",
    "\n",
    "    def __len__(self):\n",
    "        NotImplemented\n",
    "    \n",
    "    def __getitem__(\n",
    "            self, \n",
    "            idx: int,\n",
    "            ) -> Tuple[torch.FloatTensor, torch.FloatTensor]: # (5,), (1,)\n",
    "        \"\"\"\n",
    "        Returns a tuple of (x, y) where x is the input data and y is the target label.\n",
    "        shape of x: (5,)\n",
    "        shape of y: (1,)\n",
    "        \"\"\"\n",
    "        NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd4aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the shapes and dtypes\n",
    "\n",
    "data_path = './data/train.npy'\n",
    "dataset = AirFoilDataset(data_path)\n",
    "\n",
    "for idx in np.random.randint(0, len(dataset), 5):\n",
    "    x, y = dataset[idx]\n",
    "    assert x.dtype == torch.float32\n",
    "    assert y.dtype == torch.float32\n",
    "    assert x.shape == (5,)\n",
    "    assert y.shape == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af448e4f",
   "metadata": {},
   "source": [
    "## Implement the model (30)\n",
    "\n",
    "Implement your model class. Try to make use of modules like `nn.Sequential`, `nn.ModuleList`, and `nn.ModuleDict` to define a neural network with a modifiable number of layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d004070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOGRADED\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            output_dim: int,\n",
    "            NotImplemented\n",
    "        ):\n",
    "        super().__init__()\n",
    "        NotImplemented\n",
    "        \n",
    "    \n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.FloatTensor, # (batch_size, input_dim)\n",
    "            ) -> torch.FloatTensor: # (batch_size, output_dim)\n",
    "        # you can modify properties of the data before passing it through the model!\n",
    "        NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9dc998",
   "metadata": {},
   "source": [
    "## Helper functions for tracking model performance\n",
    "\n",
    "Before moving on to training, we provide an evaluation function for you to use during training. At the end of each epoch, use this function to calculate the loss on your training and validation dataset. Also, we provide a class to keep track of your losses with an option to plot the learning curve in real-time during training in the util file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b89247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL!\n",
    "\n",
    "# The first line is called a function decorator. It's a shorthand way to wrap a function with another function.\n",
    "\n",
    "# Remember that torch always keeps track of the computations so we can calculate the gradients if we want to\n",
    "# This can induce unnecessary overhead when we are not training!\n",
    "# By using this function decorator, we are telling torch that we are not interested in keeping track of gradients.\n",
    "# This can make the code run faster.\n",
    "\n",
    "@torch.inference_mode() # this is a function decorator\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    loss_fn = nn.MSELoss(reduction='sum'),\n",
    "    device = Device,\n",
    "    ):\n",
    "    \n",
    "    # Set the model to evaluation mode and move to the correct device\n",
    "    # (because some layers like dropout or batchnorm have different behavior when training and evaluating)\n",
    "    model.eval().to(device)\n",
    "\n",
    "    total_loss = 0.\n",
    "    n_samples = len(dataloader.dataset)\n",
    "    for x, y in dataloader:\n",
    "\n",
    "        # move data to the correct device and calculate the predictions\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        # calculate the loss\n",
    "        total_loss += loss_fn(y_pred, y).item() # use .item() to extract the loss as a normal python scalar\n",
    "\n",
    "    average_loss = total_loss / n_samples\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93e179",
   "metadata": {},
   "source": [
    "## Helper functions for evaluation and tracking model performance\n",
    "\n",
    "Before moving on to training, we provide an evaluation function for you to use during training. At the end of each epoch, use this function to calculate the loss on your training and validation dataset. Also, we provide a class to keep track of your losses with an option to plot the learning curve in real-time during training in the util file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b833fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train function, we use this decorator to make sure that torch keeps track of the gradients.\n",
    "# Although this is the default behavior, it's good practice to make it explicit.\n",
    "@torch.enable_grad() \n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_data: Dataset,\n",
    "    val_data: Dataset,\n",
    "\n",
    "    # training hyperparameters:\n",
    "    n_epochs: int,\n",
    "    batch_size: int,\n",
    "    opt_name: str, # Name of the optimizer class in torch.optim\n",
    "    opt_config: dict = {}, # default setting. You can pass more options to the optimizer\n",
    "    lr_scheduler_name: Union[str, None] = None, # Name of the learning rate scheduler class in torch.optim.lr_scheduler. If None, no scheduler is used\n",
    "    lr_scheduler_config: dict = {}, # default setting. You can pass more options to the scheduler\n",
    "    \n",
    "    device = Device,\n",
    "    plot_freq = None,\n",
    "    ):\n",
    "\n",
    "    loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    # initialize a learning curve tracker\n",
    "    lct = Learning_Curve_Tracker(n_epochs, plot_freq)\n",
    "\n",
    "    # create dataloaders\n",
    "    NotImplemented\n",
    "\n",
    "    # define your optimizer and learning rate scheduler.\n",
    "    # use the getattr fuction or the .__getattribute__ method to get the optimizer class from torch.optim\n",
    "    # For example, getattr(optim, 'Adam') or optim.__getattribute__('Adam') gives you optim.Adam\n",
    "    # pass their config dictionaries using ** to unpack it as keyword arguments\n",
    "    NotImplemented\n",
    "\n",
    "    epoch_pbar = tqdm(range(1, n_epochs+1), desc='Epochs', unit='epoch', leave=False, ncols=100)\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "\n",
    "        # Each epoch will be fast. No need for a progres bar inside the epoch for train or test batches!\n",
    "        # loop over training batches using the dataloader to traing the model\n",
    "        NotImplemented\n",
    "        \n",
    "        # After the epoch is done, evaluate the model on the training and validation set\n",
    "        NotImplemented\n",
    "\n",
    "        # update the learning curve tracker and the learning rate scheduler\n",
    "        NotImplemented\n",
    "\n",
    "        pass\n",
    "\n",
    "    return lct.get_losses()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0791c",
   "metadata": {},
   "source": [
    "## train your model (10)\n",
    "\n",
    "You have find a good set of hyperparameters for your model and your trianing. You will submit the successful config and state_dict. 10 points of your score depends on your model's performance on the test dataset, which will be evaluated by the autograder. Please run the final cell to save the model config and state to include them in your submission to Gradescope. Your score based on test loss will be:\n",
    "\n",
    "- `loss <= 0.035`: 15 points (5 extra points)\n",
    "\n",
    "- `0.035 < loss <= 0.05`: 10 points\n",
    "\n",
    "- `0.05 < loss <= 0.07`: 5 points\n",
    "\n",
    "- `loss > 0.07`: 0 points\n",
    "\n",
    "Hyperparameters you can explore:\n",
    "\n",
    "- model configuration: Try changing the model size like number of layers or hidden dimensions.\n",
    "\n",
    "- optimizer: Look into the [online documentation](https://pytorch.org/docs/stable/optim.html) for different choices for the optimizer, as well as their hyperparameters and regularization options.\n",
    "\n",
    "- learning rate scheduler: Look into the [online documentation](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) for different choices of schedulers for the learning rate of the optimizer and its hyperparameters, and how to use it.\n",
    "\n",
    "- training hyperparameters: You can also try increasing the number of epochs or the batch size. Training the model for more epochs may resolve underfitting. Bigger batch size may also help with training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "if __name__ == '__main__':\n",
    "    train_data = AirFoilDataset('data/train.npy')\n",
    "    val_data = AirFoilDataset('data/val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417295c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "# You can iterate using this cell to find the best configuration\n",
    "\n",
    "model_config = dict(\n",
    "    NotImplemented\n",
    ")\n",
    "\n",
    "train_config = dict(\n",
    "    NotImplemented\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7062632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "if __name__ == '__main__':\n",
    "    model = Model(**model_config).to(Device)\n",
    "    losses = train(model, train_data, val_data, **train_config, plot_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b8aac",
   "metadata": {},
   "source": [
    "## Explain your findings (10)\n",
    "Please explain how you searched for your hyperparameters, and what you learned about the effect of each in the next markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89caaafd",
   "metadata": {},
   "source": [
    "RESPONSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34479552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RUN THIS CELL TO SAVE CONFIGS AND MODEL STATE FOR YOUR SUBMISSION\n",
    "\"\"\"\n",
    "def load_model(\n",
    "        model_class,\n",
    "        config: dict, \n",
    "        state_dict: dict,\n",
    "        ):\n",
    "    model: nn.Module = model_class(**config).cpu()\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    save_yaml(model_config, 'model_config.yaml')\n",
    "    save_yaml(train_config, 'train_config.yaml')\n",
    "    torch.save(model.cpu().state_dict(), 'state_dict.pth')\n",
    "\n",
    "    # TESTING IF MODEL CAN BE LOADED WITHOUT ERRORS\n",
    "    model = load_model(\n",
    "        model_class = Model,\n",
    "        config = load_yaml('model_config.yaml'),\n",
    "        state_dict = torch.load('state_dict.pth', map_location='cpu')\n",
    "    )\n",
    "    print('Model can be loaded successfully!')\n",
    "\n",
    "    # You may encounter errors when loading the model config from the yaml file.\n",
    "    # If so, make sure all arguments are defined as basic python data structures like int, float, str, list, dict, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c2432",
   "metadata": {},
   "source": [
    "# Zip submission files\n",
    "\n",
    "You can run the following cell to zip the generated files for submission.\n",
    "\n",
    "If you are on Colab, make sure to download and then upload a completed copy of the notebook to the working directory so the code can detect and include it in the zip file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e73cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_zip = ['HW2.ipynb', 'model_config.yaml', 'train_config.yaml', 'state_dict.pth']\n",
    "output_zip = 'HW2_submission.zip'\n",
    "zip_files(output_zip, *files_to_zip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
